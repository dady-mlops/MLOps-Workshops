type: task
name: llama31-task-vllm

python: "3.10"

env:
  - HUGGING_FACE_HUB_TOKEN
  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct
  - MAX_MODE_LEN=4096

commands:
  - pip install vllm
  - vllm serve $MODEL_ID
    --tensor-parallel-size $DSTACK_GPUS_NUM
    --max-model-len $MAX_MODEL_LEN

ports: [8000]

spot_policy: on-demand
regions: [us-east-2]
instance_types: [p3.2xlarge]
